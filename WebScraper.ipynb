{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries. \n",
    "(I will use BeautifulSoup and requests libraries to extract data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import xlrd\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import re\n",
    "import csv\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import textstat\n",
    "from textstat.textstat import textstatistics,legacy_round\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing dictionary and preprocessing its data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Seq_num</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Word Proportion</th>\n",
       "      <th>Average Proportion</th>\n",
       "      <th>Std Dev</th>\n",
       "      <th>Doc Count</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Uncertainty</th>\n",
       "      <th>Litigious</th>\n",
       "      <th>Strong_Modal</th>\n",
       "      <th>Weak_Modal</th>\n",
       "      <th>Constraining</th>\n",
       "      <th>Complexity</th>\n",
       "      <th>Syllables</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ABANDON</td>\n",
       "      <td>10</td>\n",
       "      <td>118075</td>\n",
       "      <td>5.381685e-06</td>\n",
       "      <td>4.661541e-06</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>62949</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ABANDONED</td>\n",
       "      <td>11</td>\n",
       "      <td>229309</td>\n",
       "      <td>1.045157e-05</td>\n",
       "      <td>1.090675e-05</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>105201</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ABANDONING</td>\n",
       "      <td>12</td>\n",
       "      <td>20482</td>\n",
       "      <td>9.335394e-07</td>\n",
       "      <td>8.512342e-07</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>12810</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ABANDONMENT</td>\n",
       "      <td>13</td>\n",
       "      <td>276244</td>\n",
       "      <td>1.259079e-05</td>\n",
       "      <td>1.250980e-05</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>91251</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ABANDONMENTS</td>\n",
       "      <td>14</td>\n",
       "      <td>14874</td>\n",
       "      <td>6.779350e-07</td>\n",
       "      <td>9.385420e-07</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>6309</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86080</th>\n",
       "      <td>WRONGDOING</td>\n",
       "      <td>86083</td>\n",
       "      <td>67620</td>\n",
       "      <td>3.082020e-06</td>\n",
       "      <td>2.696846e-06</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>40496</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86081</th>\n",
       "      <td>WRONGDOINGS</td>\n",
       "      <td>86084</td>\n",
       "      <td>770</td>\n",
       "      <td>3.509547e-08</td>\n",
       "      <td>3.213955e-08</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>560</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86085</th>\n",
       "      <td>WRONGFUL</td>\n",
       "      <td>86088</td>\n",
       "      <td>117068</td>\n",
       "      <td>5.335787e-06</td>\n",
       "      <td>3.768259e-06</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>60105</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86086</th>\n",
       "      <td>WRONGFULLY</td>\n",
       "      <td>86089</td>\n",
       "      <td>36988</td>\n",
       "      <td>1.685859e-06</td>\n",
       "      <td>1.305931e-06</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>23931</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86094</th>\n",
       "      <td>WRONGLY</td>\n",
       "      <td>86097</td>\n",
       "      <td>1997</td>\n",
       "      <td>9.102032e-08</td>\n",
       "      <td>1.076164e-07</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>1509</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2709 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Word  Seq_num  Word Count  Word Proportion  Average Proportion  \\\n",
       "9           ABANDON       10      118075     5.381685e-06        4.661541e-06   \n",
       "10        ABANDONED       11      229309     1.045157e-05        1.090675e-05   \n",
       "11       ABANDONING       12       20482     9.335394e-07        8.512342e-07   \n",
       "12      ABANDONMENT       13      276244     1.259079e-05        1.250980e-05   \n",
       "13     ABANDONMENTS       14       14874     6.779350e-07        9.385420e-07   \n",
       "...             ...      ...         ...              ...                 ...   \n",
       "86080    WRONGDOING    86083       67620     3.082020e-06        2.696846e-06   \n",
       "86081   WRONGDOINGS    86084         770     3.509547e-08        3.213955e-08   \n",
       "86085      WRONGFUL    86088      117068     5.335787e-06        3.768259e-06   \n",
       "86086    WRONGFULLY    86089       36988     1.685859e-06        1.305931e-06   \n",
       "86094       WRONGLY    86097        1997     9.102032e-08        1.076164e-07   \n",
       "\n",
       "        Std Dev  Doc Count  Negative  Positive  Uncertainty  Litigious  \\\n",
       "9      0.000033      62949      2009         0            0          0   \n",
       "10     0.000078     105201      2009         0            0          0   \n",
       "11     0.000012      12810      2009         0            0          0   \n",
       "12     0.000082      91251      2009         0            0          0   \n",
       "13     0.000023       6309      2009         0            0          0   \n",
       "...         ...        ...       ...       ...          ...        ...   \n",
       "86080  0.000024      40496      2009         0            0          0   \n",
       "86081  0.000002        560      2009         0            0          0   \n",
       "86085  0.000031      60105      2009         0            0          0   \n",
       "86086  0.000016      23931      2009         0            0          0   \n",
       "86094  0.000005       1509      2009         0            0          0   \n",
       "\n",
       "       Strong_Modal  Weak_Modal  Constraining  Complexity  Syllables  \\\n",
       "9                 0           0             0           0          3   \n",
       "10                0           0             0           0          3   \n",
       "11                0           0             0           0          4   \n",
       "12                0           0             0           0          4   \n",
       "13                0           0             0           0          4   \n",
       "...             ...         ...           ...         ...        ...   \n",
       "86080             0           0             0           0          3   \n",
       "86081             0           0             0           0          2   \n",
       "86085             0           0             0           0          2   \n",
       "86086             0           0             0           0          3   \n",
       "86094             0           0             0           0          2   \n",
       "\n",
       "          Source  \n",
       "9      12of12inf  \n",
       "10     12of12inf  \n",
       "11     12of12inf  \n",
       "12     12of12inf  \n",
       "13     12of12inf  \n",
       "...          ...  \n",
       "86080  12of12inf  \n",
       "86081  12of12inf  \n",
       "86085  12of12inf  \n",
       "86086  12of12inf  \n",
       "86094  12of12inf  \n",
       "\n",
       "[2709 rows x 17 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"textualAnalysisDependencies/LoughranMcDonald_MasterDictionary_2020.csv\")\n",
    "df.head()\n",
    "\n",
    "\n",
    "a = df.loc[(df['Negative'] !=0) | (df['Positive']!=0)]\n",
    "\n",
    "\n",
    "# b = a[['Word', 'Negative', 'Positive']]\n",
    "\n",
    "\n",
    "\n",
    "# def normal(x):\n",
    "#     j = int(x/2009)\n",
    "#     return j\n",
    "# b['total'] = -b['Negative'].apply(normal) + b['Positive'].apply(normal)\n",
    "\n",
    "# c = b['Word']\n",
    "# d = b['total']\n",
    "# x=[]\n",
    "# x1 = c.values.tolist()\n",
    "# for i in x1:\n",
    "#     new = i.lower()\n",
    "#     x.append(new)\n",
    "# y = d.values.tolist()\n",
    "\n",
    "negative = df.loc[(df['Negative'] !=0)]\n",
    "negative_words=[]\n",
    "n_words = negative['Word'].tolist()\n",
    "for i in n_words:\n",
    "    new = i.lower()\n",
    "    negative_words.append(new)\n",
    "\n",
    "positive = df.loc[(df['Positive'] !=0)]\n",
    "positive_words=[]\n",
    "p_words = positive['Word'].tolist()\n",
    "for i in p_words:\n",
    "    new = i.lower()\n",
    "    positive_words.append(new)\n",
    "\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(word):\n",
    "    return [char for char in word]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "complex = df.loc[(df['Syllables'] > 2)]\n",
    "complex_words=[]\n",
    "c_words = complex['Word'].tolist()\n",
    "for i in c_words:\n",
    "    new = i.lower()\n",
    "    complex_words.append(new)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Articles from website list given in excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "#import xlrd\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "loc = (\"OutputExcel.xlsx\")\n",
    "\n",
    "data = pd.read_excel(loc)\n",
    "\n",
    "\n",
    "\n",
    "#wb = xlrd.open_workbook(loc)\n",
    "#sheet = wb.sheet_by_index(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:55.0) Gecko/20100101 Firefox/55.0',\n",
    "}\n",
    " \n",
    "def getIdLink(x):\n",
    "    index = int(x)\n",
    "    URL_ID = int(data.iat[x,0])\n",
    "    link = str(data.iat[x,1])\n",
    "    \n",
    "    return URL_ID, link\n",
    "\n",
    "\n",
    "a,b = data.shape\n",
    "rows = a-1\n",
    "\n",
    "\n",
    "for x in range(rows):\n",
    "    id, link = getIdLink(x)\n",
    "    \n",
    "    html_text = requests.get(link, headers = headers).text\n",
    "    soup = BeautifulSoup(html_text, 'lxml')\n",
    "\n",
    "    title = soup.find('h1', class_=\"entry-title\").text\n",
    "    articles = soup.find('div', class_=\"td-post-content\").text\n",
    "    print(title)\n",
    "    print(articles)\n",
    "    \n",
    "    textTitle = str(id)\n",
    "    file = open('articles/'+ textTitle +'.txt','w', errors=\"ignore\")\n",
    "    file.write(articles)\n",
    "    file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the scraped and saved data by removing puntuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwhen people hear ai they often think about sentient robots and magic boxes  ai today is much more mundane and simple but that doesn t mean it s not powerful  another misconception is that high profile research projects can be applied directly to any business situation  ai done right can create an extreme return on investments  rois  for instance through automation or precise prediction  but it does take thought  time  and proper implementation  we have seen that success and value generated by ai projects are increased when there is a grounded understanding and expectation of what the technology can deliver from the c suite down \\n“artificial intelligence  ai  is a science and a set of computational technologies that are inspired by but typically operate quite differently from the ways people use their nervous systems and bodies to sense  learn  reason and take action ”3 lately there has been a big rise in the day to day use of machines powered by ai  these machines are wired using cross disciplinary approaches based on mathematics  computer science  statistics  psychology  and more 4 virtual assistants are becoming more common  most of the web shops predict your purchases  many companies make use of chatbots in their customer service and many companies use algorithms to detect fraud \\nai and deep learning technology employed in office entry systems will bring proper time tracking of each employee  as this system tries to learn each person with an image processing technology whose data is feed forwarded to a deep learning model where deep learning isn t an algorithm per se  but rather a family of algorithms that implements deep networks  many layers   these networks are so deep that new methods of computation  such as graphics processing units  gpus   are required to train them  in addition to clusters of compute nodes  so using deep learning we can take detect the employee using face and person recognition scan and through which login  logout timing is recorded  using an ai system we can even identify each employee s entry time  their working hours  non working hours by tracking the movement of an employee in the office so that system can predict and report hr for the salary for each employee based on their working hours  our system can take feed from cctv to track movements of employees and this system is capable of recognizing a person even he she is being masked as in this pandemic situation by taking their iris scan  with this system installed inside the office  the following are some of the benefits \\n1 compliance litigation needs\\nfor several countries  regulations insist that the employer must keep documents available that can demonstrate the working hours performed by each employee  in the event of control from the labor inspectorate or a dispute with an employee  the employer must be able to explain and justify the working hours for the company  this can be made easy as our system is tracking employee movements\\n2 information security needs\\nthis is about monitoring user connection times to detect suspicious access times  in the event where compromised credentials are used to log on at 3 a m  on a saturday  a notification on this access could alert the it team that an attack is possibly underway \\n3 employee login logout software\\nto manage and react to employees  attendance  overtime thresholds  productivity  and suspicious access times  our system records and stores detailed and interactive reporting on users  connection times  these records allow you to better manage users  connection times and provide accurate  detailed data required by management \\n4 if you want to avoid paying overtime  make sure that your employees respect certain working time quotas or even avoid suspicious access  our system will alert the hr officer about each employee s office in and out time so that they can accordingly take action \\n5 last but not least it reduces human resource needs to keep track of the records and sending the report to hr and hr officials has to check through the report so this system will reduce times and human resource needs\\nwith the use of ai and deep learning technologies  we can automate some routines stuff with more functionality which humans need more resources to keep track thereby reducing time spent on manual data entry works rather companies can think of making their position high in the competitive world \\nblackcoffer insights 33  suriya e  vellore institute of technology\\n'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileObject = open(\"articles/1.txt\", \"r\")\n",
    "data = fileObject.read().lower()\n",
    "\n",
    "sent_tok = []\n",
    "\n",
    "text = data\n",
    "dataInSent = (sent_tokenize(text))\n",
    "sentenceLen = len(dataInSent)\n",
    "\n",
    "\n",
    "punc = '''!()-[]{};:-'—’\"-\\,<>./?@#$%^&*_~'''\n",
    " \n",
    "\n",
    "for ele in data:\n",
    "    if ele in punc:\n",
    "       data = data.replace(ele, \" \")\n",
    " \n",
    "\n",
    "raw_data=str(data).split()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not', 'ain', 'in', 'just', 'between', 'of', 'too', \"shan't\", 'to', \"you've\", 'before', 'don', 'our', 'have', 't', 'hasn', 'most', 'only', 'but', 'after', 'than', 'down', 'or', \"don't\", \"wasn't\", 'will', 'himself', 'this', 'doesn', 'haven', 'couldn', 'out', \"doesn't\", 'once', 'until', 'some', \"haven't\", 'i', 'be', \"you'd\", 'as', 'been', 'by', 'above', 'll', 'mightn', \"should've\", 'themselves', 'her', 'then', 'same', \"mightn't\", \"hadn't\", 'mustn', 'me', 'are', 'with', \"wouldn't\", 'about', 'nor', 'do', 'from', 'they', 'and', 'both', 'can', 'his', 'him', 'there', 'when', 'that', 'few', 'aren', 'we', 'he', 'during', 'hadn', \"weren't\", 'your', 'all', 'she', 'it', 'wouldn', 'theirs', 'very', 've', 'why', 'whom', 'an', 'shouldn', 'what', 'am', 'ma', 'who', 'their', 'off', 'while', 'being', \"shouldn't\", 'is', \"it's\", 'them', 'yourself', 'on', 'any', 'should', 'up', 'own', 'because', 'shan', \"you'll\", 'myself', 'a', \"didn't\", \"that'll\", 'ours', 'below', 'herself', 'further', 's', 'o', \"aren't\", \"couldn't\", 'hers', 'm', 'at', 'more', \"needn't\", 'these', 'those', 'were', 'did', 'does', \"mustn't\", 'itself', \"isn't\", \"won't\", 'yours', 'again', 'isn', 'my', 'how', 'such', 'y', 'so', \"she's\", 'ourselves', 'no', \"hasn't\", 'each', 'had', 'yourselves', 'now', 'needn', 'weren', 'wasn', 'its', 'if', 'won', 'here', \"you're\", 'under', 'you', 'was', 'doing', 'over', 'which', 'the', 'other', 'through', 'd', 're', 'having', 'has', 'didn', 'into', 'where', 'for', 'against']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "en_stopword = []\n",
    "for string in en_stopwords:\n",
    "    en_stopword.append(string.lower())\n",
    "\n",
    "\n",
    "\n",
    "print(en_stopword)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['people',\n",
       " 'hear',\n",
       " 'ai',\n",
       " 'often',\n",
       " 'think',\n",
       " 'sentient',\n",
       " 'robots',\n",
       " 'magic',\n",
       " 'boxes',\n",
       " 'ai',\n",
       " 'today',\n",
       " 'much',\n",
       " 'mundane',\n",
       " 'simple',\n",
       " 'mean',\n",
       " 'powerful',\n",
       " 'another',\n",
       " 'misconception',\n",
       " 'high',\n",
       " 'profile',\n",
       " 'research',\n",
       " 'projects',\n",
       " 'applied',\n",
       " 'directly',\n",
       " 'business',\n",
       " 'situation',\n",
       " 'ai',\n",
       " 'done',\n",
       " 'right',\n",
       " 'create',\n",
       " 'extreme',\n",
       " 'return',\n",
       " 'investments',\n",
       " 'rois',\n",
       " 'instance',\n",
       " 'automation',\n",
       " 'precise',\n",
       " 'prediction',\n",
       " 'take',\n",
       " 'thought',\n",
       " 'time',\n",
       " 'proper',\n",
       " 'implementation',\n",
       " 'seen',\n",
       " 'success',\n",
       " 'value',\n",
       " 'generated',\n",
       " 'ai',\n",
       " 'projects',\n",
       " 'increased',\n",
       " 'grounded',\n",
       " 'understanding',\n",
       " 'expectation',\n",
       " 'technology',\n",
       " 'deliver',\n",
       " 'c',\n",
       " 'suite',\n",
       " '“artificial',\n",
       " 'intelligence',\n",
       " 'ai',\n",
       " 'science',\n",
       " 'set',\n",
       " 'computational',\n",
       " 'technologies',\n",
       " 'inspired',\n",
       " 'typically',\n",
       " 'operate',\n",
       " 'quite',\n",
       " 'differently',\n",
       " 'ways',\n",
       " 'people',\n",
       " 'use',\n",
       " 'nervous',\n",
       " 'systems',\n",
       " 'bodies',\n",
       " 'sense',\n",
       " 'learn',\n",
       " 'reason',\n",
       " 'take',\n",
       " 'action',\n",
       " '”3',\n",
       " 'lately',\n",
       " 'big',\n",
       " 'rise',\n",
       " 'day',\n",
       " 'day',\n",
       " 'use',\n",
       " 'machines',\n",
       " 'powered',\n",
       " 'ai',\n",
       " 'machines',\n",
       " 'wired',\n",
       " 'using',\n",
       " 'cross',\n",
       " 'disciplinary',\n",
       " 'approaches',\n",
       " 'based',\n",
       " 'mathematics',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'statistics',\n",
       " 'psychology',\n",
       " '4',\n",
       " 'virtual',\n",
       " 'assistants',\n",
       " 'becoming',\n",
       " 'common',\n",
       " 'web',\n",
       " 'shops',\n",
       " 'predict',\n",
       " 'purchases',\n",
       " 'many',\n",
       " 'companies',\n",
       " 'make',\n",
       " 'use',\n",
       " 'chatbots',\n",
       " 'customer',\n",
       " 'service',\n",
       " 'many',\n",
       " 'companies',\n",
       " 'use',\n",
       " 'algorithms',\n",
       " 'detect',\n",
       " 'fraud',\n",
       " 'ai',\n",
       " 'deep',\n",
       " 'learning',\n",
       " 'technology',\n",
       " 'employed',\n",
       " 'office',\n",
       " 'entry',\n",
       " 'systems',\n",
       " 'bring',\n",
       " 'proper',\n",
       " 'time',\n",
       " 'tracking',\n",
       " 'employee',\n",
       " 'system',\n",
       " 'tries',\n",
       " 'learn',\n",
       " 'person',\n",
       " 'image',\n",
       " 'processing',\n",
       " 'technology',\n",
       " 'whose',\n",
       " 'data',\n",
       " 'feed',\n",
       " 'forwarded',\n",
       " 'deep',\n",
       " 'learning',\n",
       " 'model',\n",
       " 'deep',\n",
       " 'learning',\n",
       " 'algorithm',\n",
       " 'per',\n",
       " 'se',\n",
       " 'rather',\n",
       " 'family',\n",
       " 'algorithms',\n",
       " 'implements',\n",
       " 'deep',\n",
       " 'networks',\n",
       " 'many',\n",
       " 'layers',\n",
       " 'networks',\n",
       " 'deep',\n",
       " 'new',\n",
       " 'methods',\n",
       " 'computation',\n",
       " 'graphics',\n",
       " 'processing',\n",
       " 'units',\n",
       " 'gpus',\n",
       " 'required',\n",
       " 'train',\n",
       " 'addition',\n",
       " 'clusters',\n",
       " 'compute',\n",
       " 'nodes',\n",
       " 'using',\n",
       " 'deep',\n",
       " 'learning',\n",
       " 'take',\n",
       " 'detect',\n",
       " 'employee',\n",
       " 'using',\n",
       " 'face',\n",
       " 'person',\n",
       " 'recognition',\n",
       " 'scan',\n",
       " 'login',\n",
       " 'logout',\n",
       " 'timing',\n",
       " 'recorded',\n",
       " 'using',\n",
       " 'ai',\n",
       " 'system',\n",
       " 'even',\n",
       " 'identify',\n",
       " 'employee',\n",
       " 'entry',\n",
       " 'time',\n",
       " 'working',\n",
       " 'hours',\n",
       " 'non',\n",
       " 'working',\n",
       " 'hours',\n",
       " 'tracking',\n",
       " 'movement',\n",
       " 'employee',\n",
       " 'office',\n",
       " 'system',\n",
       " 'predict',\n",
       " 'report',\n",
       " 'hr',\n",
       " 'salary',\n",
       " 'employee',\n",
       " 'based',\n",
       " 'working',\n",
       " 'hours',\n",
       " 'system',\n",
       " 'take',\n",
       " 'feed',\n",
       " 'cctv',\n",
       " 'track',\n",
       " 'movements',\n",
       " 'employees',\n",
       " 'system',\n",
       " 'capable',\n",
       " 'recognizing',\n",
       " 'person',\n",
       " 'even',\n",
       " 'masked',\n",
       " 'pandemic',\n",
       " 'situation',\n",
       " 'taking',\n",
       " 'iris',\n",
       " 'scan',\n",
       " 'system',\n",
       " 'installed',\n",
       " 'inside',\n",
       " 'office',\n",
       " 'following',\n",
       " 'benefits',\n",
       " '1',\n",
       " 'compliance',\n",
       " 'litigation',\n",
       " 'needs',\n",
       " 'several',\n",
       " 'countries',\n",
       " 'regulations',\n",
       " 'insist',\n",
       " 'employer',\n",
       " 'must',\n",
       " 'keep',\n",
       " 'documents',\n",
       " 'available',\n",
       " 'demonstrate',\n",
       " 'working',\n",
       " 'hours',\n",
       " 'performed',\n",
       " 'employee',\n",
       " 'event',\n",
       " 'control',\n",
       " 'labor',\n",
       " 'inspectorate',\n",
       " 'dispute',\n",
       " 'employee',\n",
       " 'employer',\n",
       " 'must',\n",
       " 'able',\n",
       " 'explain',\n",
       " 'justify',\n",
       " 'working',\n",
       " 'hours',\n",
       " 'company',\n",
       " 'made',\n",
       " 'easy',\n",
       " 'system',\n",
       " 'tracking',\n",
       " 'employee',\n",
       " 'movements',\n",
       " '2',\n",
       " 'information',\n",
       " 'security',\n",
       " 'needs',\n",
       " 'monitoring',\n",
       " 'user',\n",
       " 'connection',\n",
       " 'times',\n",
       " 'detect',\n",
       " 'suspicious',\n",
       " 'access',\n",
       " 'times',\n",
       " 'event',\n",
       " 'compromised',\n",
       " 'credentials',\n",
       " 'used',\n",
       " 'log',\n",
       " '3',\n",
       " 'saturday',\n",
       " 'notification',\n",
       " 'access',\n",
       " 'could',\n",
       " 'alert',\n",
       " 'team',\n",
       " 'attack',\n",
       " 'possibly',\n",
       " 'underway',\n",
       " '3',\n",
       " 'employee',\n",
       " 'login',\n",
       " 'logout',\n",
       " 'software',\n",
       " 'manage',\n",
       " 'react',\n",
       " 'employees',\n",
       " 'attendance',\n",
       " 'overtime',\n",
       " 'thresholds',\n",
       " 'productivity',\n",
       " 'suspicious',\n",
       " 'access',\n",
       " 'times',\n",
       " 'system',\n",
       " 'records',\n",
       " 'stores',\n",
       " 'detailed',\n",
       " 'interactive',\n",
       " 'reporting',\n",
       " 'users',\n",
       " 'connection',\n",
       " 'times',\n",
       " 'records',\n",
       " 'allow',\n",
       " 'better',\n",
       " 'manage',\n",
       " 'users',\n",
       " 'connection',\n",
       " 'times',\n",
       " 'provide',\n",
       " 'accurate',\n",
       " 'detailed',\n",
       " 'data',\n",
       " 'required',\n",
       " 'management',\n",
       " '4',\n",
       " 'want',\n",
       " 'avoid',\n",
       " 'paying',\n",
       " 'overtime',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'employees',\n",
       " 'respect',\n",
       " 'certain',\n",
       " 'working',\n",
       " 'time',\n",
       " 'quotas',\n",
       " 'even',\n",
       " 'avoid',\n",
       " 'suspicious',\n",
       " 'access',\n",
       " 'system',\n",
       " 'alert',\n",
       " 'hr',\n",
       " 'officer',\n",
       " 'employee',\n",
       " 'office',\n",
       " 'time',\n",
       " 'accordingly',\n",
       " 'take',\n",
       " 'action',\n",
       " '5',\n",
       " 'last',\n",
       " 'least',\n",
       " 'reduces',\n",
       " 'human',\n",
       " 'resource',\n",
       " 'needs',\n",
       " 'keep',\n",
       " 'track',\n",
       " 'records',\n",
       " 'sending',\n",
       " 'report',\n",
       " 'hr',\n",
       " 'hr',\n",
       " 'officials',\n",
       " 'check',\n",
       " 'report',\n",
       " 'system',\n",
       " 'reduce',\n",
       " 'times',\n",
       " 'human',\n",
       " 'resource',\n",
       " 'needs',\n",
       " 'use',\n",
       " 'ai',\n",
       " 'deep',\n",
       " 'learning',\n",
       " 'technologies',\n",
       " 'automate',\n",
       " 'routines',\n",
       " 'stuff',\n",
       " 'functionality',\n",
       " 'humans',\n",
       " 'need',\n",
       " 'resources',\n",
       " 'keep',\n",
       " 'track',\n",
       " 'thereby',\n",
       " 'reducing',\n",
       " 'time',\n",
       " 'spent',\n",
       " 'manual',\n",
       " 'data',\n",
       " 'entry',\n",
       " 'works',\n",
       " 'rather',\n",
       " 'companies',\n",
       " 'think',\n",
       " 'making',\n",
       " 'position',\n",
       " 'high',\n",
       " 'competitive',\n",
       " 'world',\n",
       " 'blackcoffer',\n",
       " 'insights',\n",
       " '33',\n",
       " 'suriya',\n",
       " 'e',\n",
       " 'vellore',\n",
       " 'institute',\n",
       " 'technology']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data1 = [word for word in raw_data if word not in en_stopwords]\n",
    "raw_data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCleanedText(text):\n",
    "    \n",
    "    \n",
    "    #tokenize\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    new_tokens = [token for token in tokens if token not in en_stopwords]\n",
    "    \n",
    "    stemmed_tokens = [ps.stem(tokens) for tokens in new_tokens]\n",
    "    \n",
    "    clean_text = \"\".join(stemmed_tokens)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abbrevi',\n",
       " 'abbrevi',\n",
       " 'abbrevi',\n",
       " 'abbrevi',\n",
       " 'abbrevi',\n",
       " 'abbrevi',\n",
       " 'abductor',\n",
       " 'abductor',\n",
       " 'abjuratori',\n",
       " 'abolition',\n",
       " 'abolition',\n",
       " 'abolitionist',\n",
       " 'abolitionist',\n",
       " 'aborigin',\n",
       " 'aborigin',\n",
       " 'aborigin',\n",
       " 'aborigin',\n",
       " 'abortionist',\n",
       " 'abortionist',\n",
       " 'abscissa',\n",
       " 'abscissa',\n",
       " 'abscissa',\n",
       " 'absentmind',\n",
       " 'absentmindedli',\n",
       " 'absentminded',\n",
       " 'absentminded',\n",
       " 'absolut',\n",
       " 'absolut',\n",
       " 'absolut',\n",
       " 'absolut',\n",
       " 'absolut',\n",
       " 'absolutest',\n",
       " 'absolut',\n",
       " 'absolut',\n",
       " 'absolut',\n",
       " 'absolut',\n",
       " 'absolutist',\n",
       " 'absolutist',\n",
       " 'abstemi',\n",
       " 'abstemi',\n",
       " 'abstemi',\n",
       " 'abstemi',\n",
       " 'abstractedli',\n",
       " 'abstracted',\n",
       " 'abstracted',\n",
       " 'absurdest',\n",
       " 'absurdli',\n",
       " 'academician',\n",
       " 'academician',\n",
       " 'acceler',\n",
       " 'acceler',\n",
       " 'acceler',\n",
       " 'acceler',\n",
       " 'acceler',\n",
       " 'acceler',\n",
       " 'acceler',\n",
       " 'acceler',\n",
       " 'acceler',\n",
       " 'accentu',\n",
       " 'accentu',\n",
       " 'accentu',\n",
       " 'accentu',\n",
       " 'accentu',\n",
       " 'accentu',\n",
       " 'accentu',\n",
       " 'acceptor',\n",
       " 'accessori',\n",
       " 'accessori',\n",
       " 'accessway',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'acclimat',\n",
       " 'acclimat',\n",
       " 'acclimat',\n",
       " 'acclimat',\n",
       " 'acclimat',\n",
       " 'acclimat',\n",
       " 'accolad',\n",
       " 'accolad',\n",
       " 'accommod',\n",
       " 'accommod',\n",
       " 'accommod',\n",
       " 'accommod',\n",
       " 'accommodatingli',\n",
       " 'accommod',\n",
       " 'accommod',\n",
       " 'accommod',\n",
       " 'accompani',\n",
       " 'accompani',\n",
       " 'accompani',\n",
       " 'accompani',\n",
       " 'accompanist',\n",
       " 'accompanist',\n",
       " 'accompani',\n",
       " 'accompani',\n",
       " 'accomplic',\n",
       " 'accomplic',\n",
       " 'accomplish',\n",
       " 'accomplish',\n",
       " 'accomplish',\n",
       " 'accomplish',\n",
       " 'accomplish',\n",
       " 'accomplish',\n",
       " 'accordingli',\n",
       " 'accordion',\n",
       " 'accordionist',\n",
       " 'accordionist',\n",
       " 'accordion',\n",
       " 'accounthold',\n",
       " 'accounthold',\n",
       " 'accouter',\n",
       " 'accoutr',\n",
       " 'accoutr',\n",
       " 'accoutr',\n",
       " 'accoutr',\n",
       " 'accoutr',\n",
       " 'accredit',\n",
       " 'accredit',\n",
       " 'accredit',\n",
       " 'accredit',\n",
       " 'accredit',\n",
       " 'accredit',\n",
       " 'accretionari',\n",
       " 'accruabl',\n",
       " 'accrual',\n",
       " 'accrual',\n",
       " 'accultur',\n",
       " 'accultur',\n",
       " 'accultur',\n",
       " 'accultur',\n",
       " 'accultur',\n",
       " 'accultur',\n",
       " 'accumul',\n",
       " 'accumul',\n",
       " 'accumul',\n",
       " 'accumul',\n",
       " 'accumul',\n",
       " 'accumul',\n",
       " 'accumul',\n",
       " 'accumul',\n",
       " 'accumul',\n",
       " 'accuraci',\n",
       " 'accuraci',\n",
       " 'accusingli',\n",
       " 'accustom',\n",
       " 'accustom',\n",
       " 'accustom',\n",
       " 'accustom',\n",
       " 'acidifi',\n",
       " 'acidifi',\n",
       " 'acidifi',\n",
       " 'acidifi',\n",
       " 'acknowledg',\n",
       " 'acknowledg',\n",
       " 'acknowledg',\n",
       " 'acknowledg',\n",
       " 'acknowledg',\n",
       " 'acknowledg',\n",
       " 'acknowledg',\n",
       " 'acknowledg',\n",
       " 'acquaintanceship',\n",
       " 'acquaintanceship',\n",
       " 'acquiesc',\n",
       " 'acquiesc',\n",
       " 'acquiesc',\n",
       " 'acquiesc',\n",
       " 'acquiesc',\n",
       " 'acquiesc',\n",
       " 'acquiesc',\n",
       " 'acquiesc',\n",
       " 'acrimoni',\n",
       " 'acrimoni',\n",
       " 'acrimoni',\n",
       " 'acrimoni',\n",
       " 'acrimoni',\n",
       " 'acrimoni',\n",
       " 'acrobat',\n",
       " 'acrobat',\n",
       " 'acrobat',\n",
       " 'acrobat',\n",
       " 'acrobat',\n",
       " 'acrophobia',\n",
       " 'acrophobia',\n",
       " 'acropolis',\n",
       " 'actinium',\n",
       " 'actinium',\n",
       " 'actual',\n",
       " 'actual',\n",
       " 'actual',\n",
       " 'actual',\n",
       " 'actual',\n",
       " 'actual',\n",
       " 'actual',\n",
       " 'actual',\n",
       " 'actual',\n",
       " 'actual',\n",
       " 'actual',\n",
       " 'actuari',\n",
       " 'actuari',\n",
       " 'actuari',\n",
       " 'actuari',\n",
       " 'actuat',\n",
       " 'actuat',\n",
       " 'actuat',\n",
       " 'actuat',\n",
       " 'actuat',\n",
       " 'actuat',\n",
       " 'actuat',\n",
       " 'actuat',\n",
       " 'acupressur',\n",
       " 'acupressur',\n",
       " 'acupuncturist',\n",
       " 'acupuncturist',\n",
       " 'addenda',\n",
       " 'addendum',\n",
       " 'addendum',\n",
       " 'addibl',\n",
       " 'addressor',\n",
       " 'adequaci',\n",
       " 'adequaci',\n",
       " 'adequ',\n",
       " 'adequ',\n",
       " 'adequ',\n",
       " 'adequ',\n",
       " 'adjectiv',\n",
       " 'adjectiv',\n",
       " 'adjudicatori',\n",
       " 'adjustor',\n",
       " 'adjustor',\n",
       " 'administ',\n",
       " 'administ',\n",
       " 'administ',\n",
       " 'administ',\n",
       " 'administr',\n",
       " 'administr',\n",
       " 'administr',\n",
       " 'administr',\n",
       " 'administr',\n",
       " 'administr',\n",
       " 'administr',\n",
       " 'administr',\n",
       " 'administr',\n",
       " 'administr',\n",
       " 'admiralti',\n",
       " 'admiralti',\n",
       " 'admiringli',\n",
       " 'admittedli',\n",
       " 'admixtur',\n",
       " 'admixtur',\n",
       " 'admonish',\n",
       " 'admonish',\n",
       " 'admonish',\n",
       " 'admonish',\n",
       " 'admonish',\n",
       " 'admonish',\n",
       " 'admonitori',\n",
       " 'adulatori',\n",
       " 'adulteress',\n",
       " 'adulteress',\n",
       " 'adumbr',\n",
       " 'adumbr',\n",
       " 'adumbr',\n",
       " 'adumbr',\n",
       " 'adumbr',\n",
       " 'adumbr',\n",
       " 'advantag',\n",
       " 'advantag',\n",
       " 'advantag',\n",
       " 'advantag',\n",
       " 'advantag',\n",
       " 'advantag',\n",
       " 'adventiti',\n",
       " 'adventiti',\n",
       " 'adventur',\n",
       " 'adventur',\n",
       " 'adventur',\n",
       " 'adventur',\n",
       " 'adventur',\n",
       " 'adventuresom',\n",
       " 'adventuress',\n",
       " 'adventuress',\n",
       " 'adventur',\n",
       " 'adventur',\n",
       " 'adventur',\n",
       " 'adventur',\n",
       " 'adventur',\n",
       " 'adverbi',\n",
       " 'adverbi',\n",
       " 'adverbi',\n",
       " 'adversari',\n",
       " 'adversari',\n",
       " 'adversari',\n",
       " 'adversest',\n",
       " 'advertis',\n",
       " 'advertis',\n",
       " 'advertis',\n",
       " 'advertis',\n",
       " 'advertis',\n",
       " 'advertis',\n",
       " 'advertis',\n",
       " 'advertis',\n",
       " 'advertis',\n",
       " 'advertiz',\n",
       " 'advertiz',\n",
       " 'advertori',\n",
       " 'advertori',\n",
       " 'advisedli',\n",
       " 'advisor',\n",
       " 'advisori',\n",
       " 'advisor',\n",
       " 'advisori',\n",
       " 'advocaci',\n",
       " 'advocaci',\n",
       " 'aerialist',\n",
       " 'aerialist',\n",
       " 'aerobat',\n",
       " 'aerobat',\n",
       " 'aerodynam',\n",
       " 'aerodynam',\n",
       " 'aerodynam',\n",
       " 'aesthetic',\n",
       " 'aesthetic',\n",
       " 'affabl',\n",
       " 'affabl',\n",
       " 'affabl',\n",
       " 'affabl',\n",
       " 'affectedli',\n",
       " 'affectingli',\n",
       " 'affection',\n",
       " 'affection',\n",
       " 'affianc',\n",
       " 'affianc',\n",
       " 'affianc',\n",
       " 'affianc',\n",
       " 'affidavit',\n",
       " 'affidavit',\n",
       " 'afflatus',\n",
       " 'affluenc',\n",
       " 'affluenc',\n",
       " 'affluent',\n",
       " 'affluent',\n",
       " 'afforest',\n",
       " 'afforest',\n",
       " 'afforest',\n",
       " 'afforest',\n",
       " 'afforest',\n",
       " 'afforest',\n",
       " 'aficionado',\n",
       " 'aficionado',\n",
       " 'afterbirth',\n",
       " 'afterbirth',\n",
       " 'afterburn',\n",
       " 'afterburn',\n",
       " 'aftercar',\n",
       " 'aftercar',\n",
       " 'aftereffect',\n",
       " 'aftereffect',\n",
       " 'afterglow',\n",
       " 'afterglow',\n",
       " 'afterimag',\n",
       " 'afterimag',\n",
       " 'afterlif',\n",
       " 'aftermarket',\n",
       " 'aftermarket',\n",
       " 'aftermath',\n",
       " 'aftermath',\n",
       " 'afternoon',\n",
       " 'afternoon',\n",
       " 'aftershav',\n",
       " 'aftershav',\n",
       " 'aftershock',\n",
       " 'aftershock',\n",
       " 'aftertast',\n",
       " 'aftertast',\n",
       " 'aftertax',\n",
       " 'afterthought',\n",
       " 'afterthought',\n",
       " 'afterward',\n",
       " 'afterward',\n",
       " 'afterword',\n",
       " 'afterword',\n",
       " 'ageratum',\n",
       " 'ageratum',\n",
       " 'agglomer',\n",
       " 'agglomer',\n",
       " 'agglomer',\n",
       " 'agglomer',\n",
       " 'agglomer',\n",
       " 'agglomer',\n",
       " 'agglutin',\n",
       " 'agglutin',\n",
       " 'agglutin',\n",
       " 'agglutin',\n",
       " 'agglutin',\n",
       " 'agglutin',\n",
       " 'aggrandiz',\n",
       " 'aggrandiz',\n",
       " 'aggravatingli',\n",
       " 'aggressor',\n",
       " 'aggressor',\n",
       " 'agitprop',\n",
       " 'agitprop',\n",
       " 'agnostic',\n",
       " 'agnostic',\n",
       " 'agoni',\n",
       " 'agonist',\n",
       " 'agonist',\n",
       " 'agonizingli',\n",
       " 'agoni',\n",
       " 'agoraphobia',\n",
       " 'agoraphobia',\n",
       " 'agoraphob',\n",
       " 'agoraphob',\n",
       " 'agrarian',\n",
       " 'agrarian',\n",
       " 'agrarian',\n",
       " 'agrarian',\n",
       " 'agricultur',\n",
       " 'agriculturalist',\n",
       " 'agriculturalist',\n",
       " 'agricultur',\n",
       " 'agricultur',\n",
       " 'agricultur',\n",
       " 'agriculturist',\n",
       " 'agriculturist',\n",
       " 'agronomist',\n",
       " 'agronomist',\n",
       " 'airworthi',\n",
       " 'airworthiest',\n",
       " 'airworthi',\n",
       " 'airworthi',\n",
       " 'airworthi',\n",
       " 'albacor',\n",
       " 'albacor',\n",
       " 'albatross',\n",
       " 'albatross',\n",
       " 'albino',\n",
       " 'albino',\n",
       " 'albumen',\n",
       " 'albumen',\n",
       " 'albumin',\n",
       " 'albumin',\n",
       " 'albumin',\n",
       " 'alchemi',\n",
       " 'alchemi',\n",
       " 'alcohol',\n",
       " 'alcohol',\n",
       " 'alcohol',\n",
       " 'alcohol',\n",
       " 'alcohol',\n",
       " 'alcohol',\n",
       " 'alcohol',\n",
       " 'alderman',\n",
       " 'aldermen',\n",
       " 'alderwoman',\n",
       " 'alderwomen',\n",
       " 'alfalfa',\n",
       " 'alfalfa',\n",
       " 'alfresco',\n",
       " 'algebra',\n",
       " 'algebra',\n",
       " 'algebra',\n",
       " 'algebra',\n",
       " 'algorithm',\n",
       " 'algorithm',\n",
       " 'algorithm',\n",
       " 'alibi',\n",
       " 'alibi',\n",
       " 'alibi',\n",
       " 'alibi',\n",
       " 'aliment',\n",
       " 'alimentari',\n",
       " 'aliment',\n",
       " 'aliment',\n",
       " 'aliment',\n",
       " 'alkali',\n",
       " 'alkali',\n",
       " 'alkalin',\n",
       " 'alkalin',\n",
       " 'alkalin',\n",
       " 'alkali',\n",
       " 'alkaloid',\n",
       " 'alkaloid',\n",
       " 'allegi',\n",
       " 'allegi',\n",
       " 'allegor',\n",
       " 'allegor',\n",
       " 'allegor',\n",
       " 'allegori',\n",
       " 'allegorist',\n",
       " 'allegorist',\n",
       " 'allegori',\n",
       " 'allegretto',\n",
       " 'allegretto',\n",
       " 'allegro',\n",
       " 'allegro',\n",
       " 'allergen',\n",
       " 'allergen',\n",
       " 'allergen',\n",
       " 'allergi',\n",
       " 'allergist',\n",
       " 'allergist',\n",
       " 'allergi',\n",
       " 'allevi',\n",
       " 'allevi',\n",
       " 'allevi',\n",
       " 'allevi',\n",
       " 'allevi',\n",
       " 'allevi',\n",
       " 'alleyway',\n",
       " 'alleyway',\n",
       " 'allogen',\n",
       " 'allograft',\n",
       " 'allograft',\n",
       " 'alluringli',\n",
       " 'alluvia',\n",
       " 'alluvi',\n",
       " 'alluvi',\n",
       " 'alluvium',\n",
       " 'alluvium',\n",
       " 'alphabet',\n",
       " 'alphabet',\n",
       " 'alphabet',\n",
       " 'alphabet',\n",
       " 'alphabet',\n",
       " 'alphabet',\n",
       " 'alphabet',\n",
       " 'alphabet',\n",
       " 'alphabet',\n",
       " 'alphabet',\n",
       " 'alphabet',\n",
       " 'alphabet',\n",
       " 'alphabet',\n",
       " 'alphanumer',\n",
       " 'alphanumer',\n",
       " 'alphanumer',\n",
       " 'alreadi',\n",
       " 'altarpiec',\n",
       " 'altarpiec',\n",
       " 'altitud',\n",
       " 'altitud',\n",
       " 'altogeth',\n",
       " 'altruism',\n",
       " 'altruism',\n",
       " 'altruist',\n",
       " 'altruist',\n",
       " 'altruist',\n",
       " 'altruist',\n",
       " 'alumina',\n",
       " 'alumina',\n",
       " 'aluminium',\n",
       " 'aluminium',\n",
       " 'amanuensi',\n",
       " 'amateur',\n",
       " 'amateurish',\n",
       " 'amateurishli',\n",
       " 'amateurish',\n",
       " 'amateurish',\n",
       " 'amateur',\n",
       " 'amateur',\n",
       " 'amateur',\n",
       " 'amatori',\n",
       " 'amazonian',\n",
       " 'ambassador',\n",
       " 'ambassadori',\n",
       " 'ambassador',\n",
       " 'ambassadorship',\n",
       " 'ambassadorship',\n",
       " 'ambassadress',\n",
       " 'ambassadress',\n",
       " 'ambergri',\n",
       " 'ambergris',\n",
       " 'ambidexter',\n",
       " 'ambidexter',\n",
       " 'ambidextr',\n",
       " 'ambidextr',\n",
       " 'ambienc',\n",
       " 'ambienc',\n",
       " 'ambient',\n",
       " 'ambulatori',\n",
       " 'ambulatori',\n",
       " 'ambuscad',\n",
       " 'ambuscad',\n",
       " 'ambuscad',\n",
       " 'ambuscad',\n",
       " 'americium',\n",
       " 'americium',\n",
       " 'ammonia',\n",
       " 'ammonia',\n",
       " 'ammonium',\n",
       " 'ammunit',\n",
       " 'ammunit',\n",
       " 'amnesia',\n",
       " 'amnesiac',\n",
       " 'amnesiac',\n",
       " 'amnesia',\n",
       " 'amniocentes',\n",
       " 'amniocentesi',\n",
       " 'amontillado',\n",
       " 'amontillado',\n",
       " 'amperag',\n",
       " 'amperag',\n",
       " 'ampersand',\n",
       " 'ampersand',\n",
       " 'amphibian',\n",
       " 'amphibian',\n",
       " 'amphitheat',\n",
       " 'amphitheat',\n",
       " 'amphitheatr',\n",
       " 'amphitheatr',\n",
       " 'amphora',\n",
       " 'amphora',\n",
       " 'amphora',\n",
       " 'amplifi',\n",
       " 'amplifi',\n",
       " 'amplifi',\n",
       " 'amplifi',\n",
       " 'amplifi',\n",
       " 'amplifi',\n",
       " 'amplitud',\n",
       " 'amplitud',\n",
       " 'anaconda',\n",
       " 'anaconda',\n",
       " 'anaesthesia',\n",
       " 'anaesthesia',\n",
       " 'analgesia',\n",
       " 'analgesia',\n",
       " 'analyst',\n",
       " 'analyst',\n",
       " 'analyt',\n",
       " 'analyt',\n",
       " 'analyt',\n",
       " 'analyt',\n",
       " 'analyz',\n",
       " 'analyz',\n",
       " 'analyz',\n",
       " 'analyz',\n",
       " 'analyz',\n",
       " 'analyz',\n",
       " 'analyz',\n",
       " 'anarchi',\n",
       " 'anarchist',\n",
       " 'anarchist',\n",
       " 'anarchist',\n",
       " 'anarchi',\n",
       " 'anathema',\n",
       " 'anathema',\n",
       " 'anathemat',\n",
       " 'anathemat',\n",
       " 'anathemat',\n",
       " 'anathemat',\n",
       " 'ancestor',\n",
       " 'ancestor',\n",
       " 'ancestr',\n",
       " 'ancestr',\n",
       " 'ancestress',\n",
       " 'ancestress',\n",
       " 'ancestri',\n",
       " 'ancestri',\n",
       " 'anchorag',\n",
       " 'anchorag',\n",
       " 'anchorman',\n",
       " 'anchormen',\n",
       " 'anchorperson',\n",
       " 'anchorperson',\n",
       " 'anchorwoman',\n",
       " 'anchorwomen',\n",
       " 'ancientest',\n",
       " 'ancillari',\n",
       " 'ancillari',\n",
       " 'androgen',\n",
       " 'androgen',\n",
       " 'androgen',\n",
       " 'androgyni',\n",
       " 'androgyn',\n",
       " 'androgyni',\n",
       " 'anecdota',\n",
       " 'anesthesia',\n",
       " 'anesthesia',\n",
       " 'anesthesiolog',\n",
       " 'anesthesiologist',\n",
       " 'anesthesiologist',\n",
       " 'anesthesiolog',\n",
       " 'angelica',\n",
       " 'angelica',\n",
       " 'angioedema',\n",
       " 'angiogenesi',\n",
       " 'angioplasti',\n",
       " 'angioplasti',\n",
       " 'angleworm',\n",
       " 'angleworm',\n",
       " 'angora',\n",
       " 'angora',\n",
       " 'angrier',\n",
       " 'angriest',\n",
       " 'angular',\n",
       " 'angular',\n",
       " 'angular',\n",
       " 'anhydrid',\n",
       " 'anhydr',\n",
       " 'animadvers',\n",
       " 'animadvers',\n",
       " 'animadvert',\n",
       " 'animadvert',\n",
       " 'animadvert',\n",
       " 'animadvert',\n",
       " 'animalcul',\n",
       " 'animalcul',\n",
       " 'animatedli',\n",
       " 'animist',\n",
       " 'animist',\n",
       " 'animist',\n",
       " 'animos',\n",
       " 'animos',\n",
       " 'animu',\n",
       " 'animus',\n",
       " 'anklebon',\n",
       " 'anklebon',\n",
       " 'annalist',\n",
       " 'annalist',\n",
       " 'annihil',\n",
       " 'annihil',\n",
       " 'annihil',\n",
       " 'annihil',\n",
       " 'annihil',\n",
       " 'annihil',\n",
       " 'annihil',\n",
       " 'annihil',\n",
       " 'anniversari',\n",
       " 'anniversari',\n",
       " 'anniversari',\n",
       " 'annoyingli',\n",
       " 'annual',\n",
       " 'annual',\n",
       " 'annual',\n",
       " 'annual',\n",
       " 'annual',\n",
       " 'annual',\n",
       " 'annual',\n",
       " 'annual',\n",
       " 'annuit',\n",
       " 'annuit',\n",
       " 'annuiti',\n",
       " 'annuit',\n",
       " 'annuit',\n",
       " 'annuit',\n",
       " 'annuit',\n",
       " 'annuiti',\n",
       " 'annular',\n",
       " 'annunci',\n",
       " 'annunci',\n",
       " 'anodyn',\n",
       " 'anodyn',\n",
       " 'answerback',\n",
       " 'antagonist',\n",
       " 'antagonist',\n",
       " 'antagonist',\n",
       " 'antagonist',\n",
       " 'antebellum',\n",
       " 'antediluvian',\n",
       " 'antelop',\n",
       " 'antelop',\n",
       " 'antenat',\n",
       " 'antenna',\n",
       " 'antenna',\n",
       " 'antenna',\n",
       " 'anterior',\n",
       " 'anteroom',\n",
       " 'anteroom',\n",
       " 'anthologist',\n",
       " 'anthologist',\n",
       " 'anthracit',\n",
       " 'anthracit',\n",
       " 'anthropocentr',\n",
       " 'anthropoid',\n",
       " 'anthropoid',\n",
       " 'anthropolog',\n",
       " 'anthropolog',\n",
       " 'anthropolog',\n",
       " 'anthropologist',\n",
       " 'anthropologist',\n",
       " 'anthropolog',\n",
       " 'anthropomorph',\n",
       " 'anthropomorph',\n",
       " 'anthropomorph',\n",
       " 'anthropomorph',\n",
       " 'anthropomorph',\n",
       " 'antiabortionist',\n",
       " 'antiabortionist',\n",
       " 'antiag',\n",
       " 'antiaircraft',\n",
       " 'antibacteri',\n",
       " 'antibacteri',\n",
       " 'antibodi',\n",
       " 'antibodi',\n",
       " 'anticanc',\n",
       " 'anticipatori',\n",
       " 'anticler',\n",
       " 'anticlimact',\n",
       " 'anticlimact',\n",
       " 'anticlimax',\n",
       " 'anticlimax',\n",
       " 'anticlin',\n",
       " 'anticlin',\n",
       " 'anticlockwis',\n",
       " 'anticoagul',\n",
       " 'anticoagul',\n",
       " 'anticommun',\n",
       " 'anticommun',\n",
       " 'anticommunist',\n",
       " 'anticommunist',\n",
       " 'anticompetit',\n",
       " 'anticyclon',\n",
       " 'anticyclon',\n",
       " 'anticyclon',\n",
       " 'antidandruff',\n",
       " 'antidemocrat',\n",
       " 'antidepress',\n",
       " 'antidepress',\n",
       " 'antidilut',\n",
       " 'antidilut',\n",
       " 'antidiscrimin',\n",
       " 'antidot',\n",
       " 'antidot',\n",
       " 'antifascist',\n",
       " 'antifascist',\n",
       " 'antifung',\n",
       " 'antihero',\n",
       " 'antihero',\n",
       " 'antihistamin',\n",
       " 'antihistamin',\n",
       " 'antikickback',\n",
       " 'antiknock',\n",
       " 'antiknock',\n",
       " 'antilabor',\n",
       " 'antilogarithm',\n",
       " 'antilogarithm',\n",
       " 'antimacassar',\n",
       " 'antimacassar',\n",
       " 'antimalari',\n",
       " 'antimalari',\n",
       " 'antimatt',\n",
       " 'antimatt',\n",
       " 'antimicrobi',\n",
       " 'antimicrobi',\n",
       " 'antimissil',\n",
       " 'antimoni',\n",
       " 'antimoni',\n",
       " 'antinuclear',\n",
       " 'antioxid',\n",
       " 'antioxid',\n",
       " 'antiparticl',\n",
       " 'antiparticl',\n",
       " 'antipasto',\n",
       " 'antipasto',\n",
       " 'antipathet',\n",
       " 'antipersonnel',\n",
       " 'antiperspir',\n",
       " 'antiperspir',\n",
       " 'antipod',\n",
       " 'antipod',\n",
       " 'antipollut',\n",
       " 'antipoverti',\n",
       " 'antipsychot',\n",
       " 'antiquarian',\n",
       " 'antiquarian',\n",
       " 'antiquarian',\n",
       " 'antiquarian',\n",
       " 'antiquari',\n",
       " 'antiquari',\n",
       " 'antisemit',\n",
       " 'antisemit',\n",
       " 'antisemit',\n",
       " 'antisens',\n",
       " 'antiseps',\n",
       " 'antisepsi',\n",
       " 'antisept',\n",
       " 'antisept',\n",
       " 'antisept',\n",
       " 'antisera',\n",
       " 'antiserum',\n",
       " 'antiserum',\n",
       " 'antislaveri',\n",
       " 'antisoci',\n",
       " 'antisoci',\n",
       " 'antispasmod',\n",
       " 'antispasmod',\n",
       " 'antisubmarin',\n",
       " 'antitakeov',\n",
       " 'antitank',\n",
       " 'antithesi',\n",
       " 'antithet',\n",
       " 'antithet',\n",
       " 'antithet',\n",
       " 'antitoxin',\n",
       " 'antitoxin',\n",
       " 'antitumor',\n",
       " 'antivivisectionist',\n",
       " 'antivivisectionist',\n",
       " 'antiwar',\n",
       " 'anxieti',\n",
       " 'anxieti',\n",
       " 'anybodi',\n",
       " 'anybodi',\n",
       " 'apathet',\n",
       " 'apathet',\n",
       " 'aperitif',\n",
       " 'aperitif',\n",
       " 'aphrodisiac',\n",
       " 'aphrodisiac',\n",
       " 'apocalyps',\n",
       " 'apocalyps',\n",
       " 'apocalypt',\n",
       " 'apologist',\n",
       " 'apologist',\n",
       " 'apostat',\n",
       " 'apostat',\n",
       " 'apostat',\n",
       " 'apostat',\n",
       " 'apostleship',\n",
       " 'apostleship',\n",
       " 'apparatu',\n",
       " 'apparatus',\n",
       " 'apparel',\n",
       " 'apparel',\n",
       " 'apparel',\n",
       " 'apparel',\n",
       " 'apparel',\n",
       " 'apparel',\n",
       " 'apparit',\n",
       " 'apparit',\n",
       " 'appealingli',\n",
       " 'appendectomi',\n",
       " 'appendectomi',\n",
       " 'appendic',\n",
       " 'appendicitis',\n",
       " 'appendix',\n",
       " 'appendix',\n",
       " 'appertain',\n",
       " 'appertain',\n",
       " 'appertain',\n",
       " 'appertain',\n",
       " 'appetizingli',\n",
       " 'applejack',\n",
       " 'applejack',\n",
       " 'applesauc',\n",
       " 'applesauc',\n",
       " 'applianc',\n",
       " 'applianc',\n",
       " 'applier',\n",
       " 'applier',\n",
       " 'apportion',\n",
       " 'apportion',\n",
       " 'apportion',\n",
       " 'appreci',\n",
       " 'appreci',\n",
       " 'appreci',\n",
       " 'appreci',\n",
       " 'appreci',\n",
       " 'appreci',\n",
       " 'appreci',\n",
       " 'appreci',\n",
       " 'appreci',\n",
       " 'appreci',\n",
       " 'appreci',\n",
       " 'appreci',\n",
       " 'appreciatori',\n",
       " 'apprehend',\n",
       " 'apprehend',\n",
       " 'apprehend',\n",
       " 'apprehend',\n",
       " 'apprehens',\n",
       " 'apprehens',\n",
       " 'apprehens',\n",
       " 'apprehens',\n",
       " 'apprehens',\n",
       " 'apprehens',\n",
       " 'apprentic',\n",
       " 'apprentic',\n",
       " 'apprenticeship',\n",
       " 'apprenticeship',\n",
       " 'appropri',\n",
       " 'appropri',\n",
       " 'appropri',\n",
       " 'appropri',\n",
       " 'appropri',\n",
       " 'appropri',\n",
       " 'appropri',\n",
       " 'appropri',\n",
       " 'appropri',\n",
       " 'appropri',\n",
       " 'appropri',\n",
       " 'approvingli',\n",
       " 'approxim',\n",
       " 'approxim',\n",
       " 'approxim',\n",
       " 'approxim',\n",
       " 'approxim',\n",
       " 'approxim',\n",
       " ...]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "new_data1=[getCleanedText(i) for i in raw_data]\n",
    "complex_words1=[getCleanedText(i) for i in complex_words]\n",
    "positive_words1=[getCleanedText(i) for i in positive_words]\n",
    "negative_words1=[getCleanedText(i) for i in negative_words]\n",
    "\n",
    "new_data1 = list(filter(None, new_data1))\n",
    "totalCleanedWords=len(new_data1)\n",
    "\n",
    "def syllables_count(word):\n",
    "    return textstatistics().syllable_count(word)\n",
    "\n",
    "complex_words2=[]\n",
    "for i in complex_words1:\n",
    "    vowelCount=syllables_count(i)\n",
    "    \n",
    "#     vowelCount=0\n",
    "#     word=(split(i))\n",
    "#     if 'a' in word:\n",
    "#         count = word.count('a')\n",
    "#         vowelCount+=1\n",
    "#     if 'e' in word:\n",
    "#         vowelCount+=1\n",
    "#     if 'i' in word:\n",
    "#         vowelCount+=1\n",
    "#     if 'o' in word:\n",
    "#         vowelCount+=1\n",
    "#     if 'u' in word:\n",
    "#         vowelCount+=1\n",
    "    if vowelCount>2:\n",
    "        complex_words2.append(i)\n",
    "complex_words2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Details like word count, number of sentences etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_sentences(raw_data):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    return list(doc.sents)\n",
    " \n",
    "# Returns Number of Words in the text\n",
    "def word_count(raw_data):\n",
    "    sentences = break_sentences(raw_data)\n",
    "    words = 0\n",
    "    for sentence in sentences:\n",
    "        words += len([token for token in sentence])\n",
    "    return words\n",
    " \n",
    "# Returns the number of sentences in the text\n",
    "def sentence_count(raw_data):\n",
    "    sentences = break_sentences(raw_data)\n",
    "    return len(sentences)\n",
    " \n",
    "# Returns average sentence length\n",
    "def avg_sentence_length(raw_data):\n",
    "    words = word_count(raw_data)\n",
    "    sentences = sentence_count(raw_data)\n",
    "    average_sentence_length = float(words / sentences)\n",
    "    return average_sentence_length\n",
    " \n",
    "# Textstat is a python package, to calculate statistics from\n",
    "# text to determine readability,\n",
    "# complexity and grade level of a particular corpus.\n",
    "# Package can be found at https://pypi.python.org/pypi/textstat\n",
    "# def syllables_count(word):\n",
    "#     return textstatistics().syllable_count(word)\n",
    " \n",
    "# Returns the average number of syllables per\n",
    "# word in the text\n",
    "def avg_syllables_per_word(raw_data):\n",
    "    syllable = syllables_count(raw_data)\n",
    "    words = word_count(raw_data)\n",
    "    ASPW = float(syllable) / float(words)\n",
    "    return legacy_round(ASPW, 1)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "### Presenting Positive Words, Negative words, Complex words and Measures frome the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ve 44 success\n",
      "+ve 64 inspir\n",
      "+ve 243 benefit\n",
      "+ve 270 abl\n",
      "+ve 277 easi\n",
      "+ve 283 inform\n",
      "+ve 335 better\n",
      "+ve 422 posit\n",
      "+ve 427 insight\n",
      "-ve 57 artifici\n",
      "-ve 67 quit\n",
      "-ve 81 late\n",
      "-ve 94 disciplinari\n",
      "-ve 123 fraud\n",
      "-ve 246 litig\n",
      "-ve 248 sever\n",
      "-ve 266 disput\n",
      "-ve 272 justifi\n",
      "-ve 291 suspici\n",
      "-ve 304 alert\n",
      "-ve 321 suspici\n",
      "-ve 361 suspici\n",
      "-ve 364 alert\n",
      "complex 17 misconcept\n",
      "complex 25 situat\n",
      "complex 42 implement\n",
      "complex 51 understand\n",
      "complex 57 artifici\n",
      "complex 58 intellig\n",
      "complex 94 disciplinari\n",
      "complex 97 mathemat\n",
      "complex 103 virtual\n",
      "complex 121 algorithm\n",
      "complex 153 algorithm\n",
      "complex 158 algorithm\n",
      "complex 159 implement\n",
      "complex 198 identifi\n",
      "complex 228 capabl\n",
      "complex 234 situat\n",
      "complex 243 benefit\n",
      "complex 245 complianc\n",
      "complex 255 document\n",
      "complex 265 inspector\n",
      "complex 272 justifi\n",
      "complex 286 monitor\n",
      "complex 291 suspici\n",
      "complex 295 compromis\n",
      "complex 296 credenti\n",
      "complex 307 possibl\n",
      "complex 308 underway\n",
      "complex 321 suspici\n",
      "complex 328 interact\n",
      "complex 361 suspici\n",
      "complex 370 accordingli\n",
      "complex 387 offici\n",
      "complex 414 manual\n",
      "complex 432 institut\n",
      "9 14 -0.21739030434782608 0.052995391582959926 18.083333333333332 0.07834101382488479 7.2646697388632875 1.5\n",
      "31.53846153846154 18.083333333333332\n"
     ]
    }
   ],
   "source": [
    "ncount=0\n",
    "pcount=0\n",
    "ccount=0\n",
    "\n",
    "for i in range(len(new_data1)): \n",
    "    if new_data1[i] in positive_words1:\n",
    "        print('+ve', i, new_data1[i] )\n",
    "        pcount += 1\n",
    "\n",
    "for i in range(len(new_data1)): \n",
    "    if new_data1[i] in negative_words1:\n",
    "        print('-ve', i, new_data1[i] )\n",
    "        ncount += 1\n",
    "        \n",
    "for i in range(len(new_data1)): \n",
    "    if new_data1[i] in complex_words2:\n",
    "        print('complex', i, new_data1[i] )\n",
    "        ccount += 1\n",
    "\n",
    "noofsen = sentence_count(data)\n",
    "noofword=word_count(data)\n",
    "\n",
    "senlen =avg_sentence_length(data)\n",
    "\n",
    "polarityScore = ((pcount-ncount)/(pcount+ncount))+0.000001\n",
    "\n",
    "subjectivityScore = (pcount + ncount)/ ((totalCleanedWords) + 0.000001)\n",
    "\n",
    "avgSentLen = totalCleanedWords/sentenceLen\n",
    "\n",
    "percentComplex = ccount/totalCleanedWords\n",
    "\n",
    "fogIndex = 0.4 * (avgSentLen + percentComplex)\n",
    "\n",
    "avgSyllables = avg_syllables_per_word(data) \n",
    "\n",
    "        \n",
    "print(pcount,ncount, polarityScore, subjectivityScore, senlen, percentComplex, fogIndex, avgSyllables )\n",
    "print(senlen,avgSentLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
